# Idempotent Producer
  - To send message exactly once to the broker and persisted(written to log).
  - props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true");
  - By enabling idempotence, the producer automatically sets acks to all
    and guarantees message delivery for the lifetime of the Producer instance.

  - Refer:
    + https://stackoverflow.com/a/60289031/1171533


# Why idempotent producer is not enough? Why we need a transactional producer?

  - Actually idemnpotency by itself does not always guarantee exactly once event delivery.
    Let's say you have a consumer that consumes an event, processes it and produces an event.
    Somewhere in this process the offset that the consumer uses must be incremented and persisted.
    Without a transactional producer, if it happens before the producer sends a message,
    the message might not be sent and its at most once delivery. If you do it after the message
    is sent you might fail in persisting the offset and then the consumer would read the same
    message again and the producer would send a duplicate, you get an at least once delivery.
    The all or nothing mechanism of a transactional producer prevents this scenario given that
    you store your offset on kafka, the new message and the incrementation of the offset of
    the consumer becomes an atomic action.


  - In scenarios where in your application thread you read and then process and then procuce.
  - Relevant to workflows in which you are consuming and then producing messages based on what you
     consumed.

  - public void sendOffsetsToTransaction​(Map<TopicPartition,​OffsetAndMetadata> offsets,
    ConsumerGroupMetadata groupMetadata)
                                  throws ProducerFencedException

    Sends a list of specified offsets to the consumer group coordinator, and also marks those offsets
    as part of the current transaction. These offsets will be considered committed only if the transaction
    is committed successfully. The committed offset should be the next message your application will consume, i.e. lastProcessedMessageOffset + 1.

    This method should be used when you need to batch consumed and produced messages together,
    typically in a consume-transform-produce pattern. Thus, the specified groupMetadata should be
     extracted from the used consumer via KafkaConsumer.groupMetadata() to leverage consumer group metadata.
     This will provide stronger fencing than just supplying the consumerGroupId and passing in new ConsumerGroupMetadata(consumerGroupId),
     however note that the full set of consumer group metadata returned by KafkaConsumer.groupMetadata()
     requires the brokers to be on version 2.5 or newer to understand.

    Note, that the consumer should have enable.auto.commit=false and should also not commit offsets manually
    (via sync or async commits). This method will raise TimeoutException if the producer cannot send offsets
     before expiration of max.block.ms. Additionally, it will raise InterruptException if interrupted.


  - Sample code generated by chatGPT. https://chatgpt.com/share/4e97c0a3-b6ae-41ef-875a-a1a4b223af08

    KafkaConsumer<String, String> consumer = new KafkaConsumer<>(consumerProps);
    KafkaProducer<String, String> producer = new KafkaProducer<>(producerProps);

    producer.initTransactions();

    consumer.subscribe(Collections.singletonList("input-topic"));

    try {
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));

            if (records.count() > 0) {
                producer.beginTransaction();

                Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();

                for (ConsumerRecord<String, String> record : records) {
                    // Process the record
                    String processedValue = processRecord(record.value());

                    // Send the processed record to another topic
                    producer.send(new ProducerRecord<>("output-topic", record.key(), processedValue));

                    // Track the offset
                    offsets.put(new TopicPartition(record.topic(), record.partition()),
                                new OffsetAndMetadata(record.offset() + 1));
                }

                // Send offsets to the transaction
                producer.sendOffsetsToTransaction(offsets, "consumer-group");

                producer.commitTransaction();
            }
        }
    } catch (WakeupException e) {
        // Handle shutdown
    } catch (Exception e) {
        producer.abortTransaction();
        e.printStackTrace();
    } finally {
        consumer.close();
        producer.close();
    }


 - Reference
  + https://stackoverflow.com/a/71130037/1171533
  + https://kafka.apache.org/30/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#sendOffsetsToTransaction(java.util.Map,org.apache.kafka.clients.consumer.ConsumerGroupMetadata)

# Transactional producer

  - By enabling transactions, the producer automatically enables
    idempotence (and acks=all). Transactions allow to group produce
    requests and offset commits and ensure all or nothing gets committed to Kafka.


  - Refer
    + https://stackoverflow.com/a/60289031/1171533




# Reference:
  - ChatGPT: https://chatgpt.com/share/4e97c0a3-b6ae-41ef-875a-a1a4b223af08