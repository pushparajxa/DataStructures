# Idempotent Producer
  - To send message exactly once to the broker and persisted(written to log).
  - props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true");
  - By enabling idempotence, the producer automatically sets acks to all
    and guarantees message delivery for the lifetime of the Producer instance.
  - Idempotence is not only acks=all, but also retries=Integer.MAX_VALUE

  - Refer:
    + https://stackoverflow.com/a/60289031/1171533
    + https://kafka.apache.org/11/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html


# Transactional producer

  - We put transactional.id in the producer properties to make it transactional.
      props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "transactional-producer-1");

  - By enabling transactions, the producer automatically enables
    idempotence (and acks=all). Transactions allow to group produce
    requests and offset commits and ensure all or nothing gets committed to Kafka.

  - When using transactions, you can configure if consumers should only see records
    from committed transactions by setting isolation.level to read_committed, otherwise by default
    they see all records including from discarded transactions.

  - In particular, the replication.factor should be at least 3, and
    the min.insync.replicas for these topics should be set to 2.

  - Finally, in order for transactional guarantees to be realized from end-to-end,
    the consumers must be configured to read only committed messages as well.

  - Refer
    + https://stackoverflow.com/a/60289031/1171533
    + https://kafka.apache.org/11/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html
      --> For transactional and idempotent producer.


# Why idempotent producer is not enough? Why we need a transactional producer?

  - Actually idemnpotency by itself does not always guarantee exactly once event delivery.
    Let's say you have a consumer that consumes an event, processes it and produces an event.
    Somewhere in this process the offset that the consumer uses must be incremented and persisted.
    Without a transactional producer, if it happens before the producer sends a message,
    the message might not be sent and its at most once delivery. If you do it after the message
    is sent you might fail in persisting the offset and then the consumer would read the same
    message again and the producer would send a duplicate, you get an at least once delivery.
    The all or nothing mechanism of a transactional producer prevents this scenario given that
    you store your offset on kafka, the new message and the incrementation of the offset of
    the consumer becomes an atomic action.


  - In scenarios where in your application thread you read and then process and then produce.
  - Relevant to workflows in which you are consuming and then producing messages based on what you
     consumed.

  - public void sendOffsetsToTransaction(Map<TopicPartition,OffsetAndMetadata> offsets,
                                         ConsumerGroupMetadata groupMetadata)
                                  throws ProducerFencedException

    Sends a list of specified offsets to the consumer group coordinator, and also marks those offsets
    as part of the current transaction. These offsets will be considered committed only if the transaction
    is committed successfully. The committed offset should be the next message your application will consume, i.e. lastProcessedMessageOffset + 1.

    This method should be used when you need to batch consumed and produced messages together,
    typically in a consume-transform-produce pattern. Thus, the specified groupMetadata should be
     extracted from the used consumer via KafkaConsumer.groupMetadata() to leverage consumer group metadata.
     This will provide stronger fencing than just supplying the consumerGroupId and passing in new ConsumerGroupMetadata(consumerGroupId),
     however note that the full set of consumer group metadata returned by KafkaConsumer.groupMetadata()
     requires the brokers to be on version 2.5 or newer to understand.

    Note, that the consumer should have enable.auto.commit=false and should also not commit offsets manually
    (via sync or async commits). This method will raise TimeoutException if the producer cannot send offsets
     before expiration of max.block.ms. Additionally, it will raise InterruptException if interrupted.


  - Sample code generated by chatGPT. https://chatgpt.com/share/4e97c0a3-b6ae-41ef-875a-a1a4b223af08

    KafkaConsumer<String, String> consumer = new KafkaConsumer<>(consumerProps);
    KafkaProducer<String, String> producer = new KafkaProducer<>(producerProps);

    producer.initTransactions();

    consumer.subscribe(Collections.singletonList("input-topic"));

    try {
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));

            if (records.count() > 0) {
                producer.beginTransaction();

                Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();

                for (ConsumerRecord<String, String> record : records) {
                    // Process the record
                    String processedValue = processRecord(record.value());

                    // Send the processed record to another topic
                    producer.send(new ProducerRecord<>("output-topic", record.key(), processedValue));

                    // Track the offset
                    offsets.put(new TopicPartition(record.topic(), record.partition()),
                                new OffsetAndMetadata(record.offset() + 1));
                }F

                // Send offsets to the transaction
                producer.sendOffsetsToTransaction(offsets, "consumer-group");

                producer.commitTransaction();
            }
        }
    } catch (WakeupException e) {
        // Handle shutdown
    } catch (Exception e) {
        producer.abortTransaction();
        e.printStackTrace();
    } finally {
        consumer.close();
        producer.close();
    }


 - Reference
  + https://stackoverflow.com/a/71130037/1171533
  + https://kafka.apache.org/30/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#sendOffsetsToTransaction(java.util.Map,org.apache.kafka.clients.consumer.ConsumerGroupMetadata)





# Reference:
  - ChatGPT: https://chatgpt.com/share/4e97c0a3-b6ae-41ef-875a-a1a4b223af08


Implementation details of Kafka transactions
=============================================

nternally, the
Kafka brokers implement exactly-once semantics through careful design of the storage layer and the introduction of new components:
the Transaction Coordinator and the use of the Transaction Log (an internal Kafka topic).

1. Broker-side for Idempotent Producers

For idempotent producers, the broker's primary role is to track and filter duplicate
messages based on unique identifiers.

    PID (Producer ID) Management:
          When an idempotent producer starts, it sends an InitProducerId request to the broker.
          A designated broker (acting as the transaction coordinator, even for just idempotence) assigns a unique,
          long-lived PID to the producer session.

    Sequence Number Tracking:
          Each broker's partition leader maintains a state for every active PID writing to it.
           This state includes the last sequence number successfully written to the log for that PID and partition.
           This information is stored durably in an internal snapshot file (.snapshot).

    Duplicate Detection:
          When a producer sends a batch of messages, it includes its PID and the sequence numbers
          for that batch. The broker checks the incoming sequence number against the last one stored.

          -> If the incoming sequence number is exactly one greater than the stored one, the broker
          accepts the request and
           appends the messages to the partition log.

         ->  If the sequence number is less than or equal to the stored one, the broker detects
           it as a duplicate (due to a producer retry) and discards the message batch without writing it
           to the log, while still sending back a success acknowledgment to the producer.

         ->  If the sequence number is significantly larger, it indicates out-of-order delivery, and
          the broker may reject the request.

    Durability:
          The producer state (PID, epoch, last sequence number) is persisted to disk to
          survive broker restarts. On restart, the broker rebuilds its in-memory state from these logs and snapshots,
          ensuring that idempotence guarantees are maintained even if the broker fails.


2. Broker-side for Transactional Producers and Atomic Commits
    Transactions involve coordination across potentially multiple topic partitions
     and use a two-phase commit protocol managed by a specialized component.

    Transaction Coordinator: Every Kafka broker runs a TransactionCoordinator module.
      Each transactional.id is mapped to a specific partition of an internal Kafka topic
      called the __transaction_state (or transaction log) using a hash function. The broker that
      is the leader for that partition is the coordinator for that specific transactional.id.


    Transaction Log: The transaction log stores the current state of all ongoing
        transactions (e.g., Ongoing, PrepareCommit, CompleteCommit). This log
        is highly available and replicated like any other Kafka topic, providing durability for transaction state.


    Two-Phase Commit Protocol:

        Producer Registration: The producer first registers its transactional.id with its
              assigned coordinator. The coordinator uses an epoch to prevent "zombie" producer instances
              from interfering with the current one.

        Message Production: Messages are sent to the respective partition leaders. Each message in a
        transaction is marked internally as belonging to an open transaction. The data is written
        to the topic-partitions immediately, but it is not yet visible to read_committed consumers.
        The list of partitions involved in the transaction is tracked in the transaction log.

        Phase 1 (Prepare Commit): When the producer calls commitTransaction(), the coordinator
        updates the state in the transaction log to PrepareCommit. Once this is logged and replicated,
        the transaction is guaranteed to be committed, regardless of further failures.

        Phase 2 (Write Markers): The coordinator then writes a special "commit marker"
        (or "abort marker" if aborted) to all topic partitions that were part of the transaction.
        These markers are control messages that consumers use for filtering.

        Complete Transaction: Once all markers are written, the coordinator updates the
        transaction state in the transaction log to CompleteCommit.
        The producer can now start a new transaction.

3. Broker-side for Consumers (read_committed)

    Filtering Logic:
        Consumers configured with isolation.level=read_committed are aware of these transaction
        markers. When a broker receives a Fetch request from such a consumer, the broker's
        response logic filters out messages belonging to open transactions (those without a commit/abort marker)
        and messages from aborted transactions. Only messages with a commit marker, or
        non-transactional messages, are returned to the
        consumer.


### Reference:

https://www.google.com/search?client=firefox-b-1-d&q=how+apache+kafka+implemented+exactly+once+semantics%3F&udm=50&ved=2ahUKEwj4vPiOle2QAxU56ckDHYnRALoQ0NsOegQIAxAB&aep=10&ntc=1&mstk=AUtExfBCjRkLDhYMYy9E_oDTIhEX5RXwvqWXlj3jnWpLaiEfhDac58atWnO-cuavM20cC4Ug5Z13EjW0oYazGvfx7Q3ThqE_3qJSQH81tBcfdz_2VhJhsA9E4A5BEOfA9rWIlhDIynZxitxdlGxTRCEM2olf4Pbad_cvzYQUaGqmDs5VqprA5cHZOxIpUcoiMyBcFY9-bkqlzP0HdbGCSy5TBJxONBT3PuX8K3ma-wYyXt8zGBD_WwjQXx-jDsqgEEyBBOROx1AbTJh0C8OUUMPO8-gMb3R-tC7TfbaWXl-lDO3Q9HytnqbWoswepBw3oRb7yYcXql2RYwxz-Q&csuir=1&mtid=2scUaeiVE4HHp84Pso6cmA4