HTTP is a single request and response protocol.

Main Reference: http://nadirmuzaffar.blogspot.com/2013/03/polling-long-polling-comet-server-side.html

HTTP Persistence Connection
----------------------------
* It is also called as HTTP Keep-alive. Using this a single TCP connection is used to send multiple
http requests. Without this for each request from client to server, a connection is opened and is
closed after server sends the response. Persistence connection enables pipelining of requests
(multiple requests over single connection)

* Refer https://en.wikipedia.org/wiki/HTTP_persistent_connection

HTTP 1.0
---------
* In HTTP/1.x versions, request and responses are sent in plain text. Images are sent converted to
  binary representation and sent as 0s and 1s

* Connections are not persistent. However un-officially it is supported by the used of header

  Connection: keep-alive

Client should send request with above header and if sever supports it then it will send response
with the same header. After this, connection is not dropped until client sends a request with the
header


  Connection:

  Connection: close


HTTP 1.1
--------
  * All connections are persistent by-default (with timeout).

  * HTTP/1.1 Spec limit number of keep-alive connections to only 2 per server by the client.

  * Timeout: connection timeout of Apache httpd 1.3 and 2.0 is as little as 15 seconds and just 5
    seconds for Apache httpd 2.2 and above.
       * Refer: https://en.wikipedia.org/wiki/HTTP_persistent_connection


  Chunked transfer encoding
  -------------------------
  Server can send response in chunks and similarly client can upload a big file in chunks.
  This helps in faster loading of webpages at client side.

  Ref:
     1. https://gist.github.com/CMCDragonkai/6bfade6431e9ffb7fe88
     2. https://youtu.be/UMwQjFzTQXw?si=cEXYfAWd2mAvmIou&t=141

  HTTP Pipelining
  ---------------
  * It is available with HTTP-1.1

  * It allows more than one requests to be sent over single TCP connection without waiting for their
  responses. Server should send responses non-pipelined, i.e., serially in the order it has received
  the requests. This may result in HOL (head of line blocking) as the initial request received by the
  server may be take lot of time than others. For example, if the data is present in the server
  cache then it can send immediately, otherwise, it has to query database and send the result while
  other requests's data is available in the cache and can be sent quickly.


  * Client sends multiple requests over the persistent connection and the server should return the
  responses in the correct order the requests are received. Server need not to handle requests
  concurrently.

  * Idempotent requests shall only be pipelined.

  * Non-Idempotent requests like POST shall not be pipelined. As per standards specification.
    - Section 8.1.1.2 Pipelining
      # https://www.w3.org/Protocols/rfc2616/rfc2616-sec8.html

  * Head of line blocking (HOL)

    - One form of HOL blocking in HTTP/1.1 is when the number of allowed parallel requests in the browser is used up,
      and subsequent requests need to wait for the former ones to complete. HTTP/2 addresses this issue through request multiplexing,
      which eliminates HOL blocking at the application layer, but HOL still exists at the transport (TCP) layer.
      HTTP/3 uses QUIC instead of TCP which removes HOL blocking in the transport layer.

     - Reference:
      # https://en.wikipedia.org/wiki/Head-of-line_blocking

  * HTTP/1.1 pipelining does not require any special headers or parameters to be set in the request.
    But client and server should support it.

  * Reference
    - https://en.wikipedia.org/wiki/HTTP_pipelining
    - https://www.youtube.com/watch?v=UMwQjFzTQXw  (Alex Wu on HTTP 1.x| HTTP 2| HTTp 3)



    Cache-Control Header
    ====================

      The Cache-Control header is used to specify directives for caching mechanisms in both requests
      and responses. It controls how, and for how long, a resource can be cached by browsers and
      intermediate caches.

        Usage in Responses

       In response headers, the Cache-Control header can include various directives:

          public: Indicates that the response can be cached by any cache.
          private: Indicates that the response is intended for a single user and should not be stored by shared caches (e.g., proxies).
          no-cache: Forces caches to submit the request to the origin server for validation before releasing a cached copy.
          no-store: Instructs caches not to store any part of the request or response.
          max-age=<seconds>: Specifies the maximum amount of time a resource is considered fresh. After this time, the resource is stale.
          must-revalidate: Indicates that once the resource becomes stale, caches must not use the stale copy without first revalidating it with the origin server.

      Example of Cache-Control in Response:
      ----------------------------------
        HTTP/1.1 200 OK
        Cache-Control: max-age=3600, public
        Content-Type: text/html

        <html>
        <body><h1>Hello, World!</h1></body>
        </html>


      Usage in Requests
      -----------------
      In request headers, Cache-Control can be used to influence the behavior of caches when the client requests resources:

          no-cache: The client is requesting a fresh copy of the resource from the server, even if the cache has a valid copy.
          max-age=<seconds>: Similar to the response, this directive can specify how long the client is willing to accept a cached response.

      Example of Cache-Control in Request:
         GET /example HTTP/1.1
         Host: www.example.com
         Cache-Control: no-cache

    Tag Header
    ====================

       The ETag (Entity Tag) header is used for web cache validation and to help manage caching behavior.
       It is a unique identifier assigned by the server to a specific version of a resource.
       When a resource changes, its ETag value changes.

        Usage

          Response ETag: The server generates an ETag when sending a response. This tag reflects the current version of the resource.
          Request ETag: The client can include the If-None-Match header in subsequent requests to check if the resource has changed.

      Example of ETag in Request:
      -----------------
        HTTP/1.1 200 OK
        ETag: "abc123"
        Content-Type: text/html

        <html>
        <body><h1>Hello, World!</h1></body>
        </html>

      Example of ETag in Request:
      -----------------
        GET /example HTTP/1.1
        Host: www.example.com
        If-None-Match: "abc123"



HTTP/2
------
  * In HTTP/2, request and response are sent in binary format


    Key Differences in HTTP/2

        Binary Format:
            Framing Layer: HTTP/2 introduces a binary framing layer, which encapsulates all HTTP messages
            in a series of frames. These frames can be of different types (e.g., headers, data, priority)
             and are encoded in a binary format. This allows for more efficient parsing and processing by both clients and servers.
            Efficiency: The binary format reduces the overhead associated with parsing plain text,
            leading to improved performance, especially for high-latency connections.

        Header Compression:
            HTTP/2 employs HPACK, a header compression format that reduces the size of header fields.
             This is particularly beneficial for reducing the amount of data sent for requests and responses,
             as headers can often be large and repetitive.

        Multiplexing:
            With the binary framing layer, HTTP/2 allows multiple streams (requests/responses)
            to be sent simultaneously over a single connection without blocking. This is a significant
            improvement over HTTP/1.x, where requests are often processed sequentially.

        Control Frames:
            In addition to data frames that carry the payload of requests and responses,
            HTTP/2 also defines control frames for managing streams, flow control, and other connection-related activities.

    Example of HTTP/2 Request and Response

        While you cannot view HTTP/2 requests and responses directly as plain text,
          here's a simplified representation of what happens under the hood:

        HTTP/2 Request: The client sends a request in a series of binary frames, which might include:
            A frame for the headers (e.g., method, URL, headers).
            A data frame (if there is a body).

        HTTP/2 Response: The server responds with its own set of binary frames, which may include:
            A frame for the response headers (e.g., status code, headers).
            A data frame containing the response body.

    HTTP/2 Streams
    -------------------
    The HTTP/2 specification defines a stream as “an independent, bidirectional sequence of frames exchanged between
    the client and server within an HTTP/2 connection.”
    You can think of a stream as a series of frames making up an individual HTTP request/response pair on a connection.
    When a client wants to make a request it initiates a new stream. The server will then reply on that same stream.
    This is similar to the request/response flow of h1 with the important difference that because of
    the framing, multiple requests and responses can interleave together without one blocking another.
    The Stream Identifier (bytes 6–9 of the frame header) is what indicates which stream a frame belongs to.


    Ref: Page 47, Book: Learning HTTP/2, A Practical guide for beginners. Tech/Networking folder.


HTTP Long Polling:
-------------------

* Long polling can be implemented in any version of HTTP.

Client makes a request. After server sends a response the transaction is complete but the
underlying (TCP) (will stay unless closed by server or client) will stay open. Then client can make
another request. Server can take any amount of time (less than or equal to timeout) to send a
response.

Refer: https://stackoverflow.com/a/31335574/1171533
Timeout: connection timeout of Apache httpd 1.3 and 2.0 is as little as 15 seconds and just 5
seconds for Apache httpd 2.2 and above.
   Refer: https://en.wikipedia.org/wiki/HTTP_persistent_connection

* Long polling code example is in the package com.html.http.longpolling


HTTP Streaming (Using HTTP 1.1)
---------------
* HTTP streaming is only involves the HTTP protocol and not websockets. Streaming is also the basis for HTML5 server sent events.

COMET
------
Client makes a request. Server never ends the response. Will keep on sending messages. End of
message is identified with an identifier.

* Refer:
  * https://en.wikipedia.org/wiki/Comet_%28programming%29
  * https://infrequently.org/wp-content/LowLatencyData.pdf
  * Comet: Low Latency Data for the Browser (Alex Russel)
    - https://infrequently.org/2006/03/comet-low-latency-data-for-the-browser/
  * http://nadirmuzaffar.blogspot.com/2013/03/polling-long-polling-comet-server-side.html


      Difference between Long polling with HTTP 1.1 vs COMET
      ------------------------------------------------------
      * In comet there is single request from the client and server never ends the response. In long
        polling client will be making requests after receiving updates from it.

      * Refer http://nadirmuzaffar.blogspot.com/2013/03/polling-long-polling-comet-server-side.html

      * https://stackoverflow.com/questions/892287/advantage-of-comet-over-long-request-polling?rq=1
            [Advantage of COMET over long request polling?]


      Buffering problem with COMET and Long polling and HTTP Streaming
      ----------------------------------------------------------------
      * https://gist.github.com/CMCDragonkai/6bfade6431e9ffb7fe88

      Difference between HTTP Streaming and COMET
      -------------------------------------------
      * If you don't know the amount of data that you are going to send, i.e., live stream of a
        match, you send the data in packets/chunks, which what HTTP Streaming about. In COMET you are
        sending updates to the client from the server through single open connection.



Server Sent Events
------------------
* COMET and HTTP Streams suffer the problem of buffering. SSE will eliminate that by sending HTTP ACKs.
* Content type shall be set as below
    * Content-type: text/event-stream
* Limitations of SSE
  * Maximum open connections, which can be specially painful when opening various tabs as the limit is per browser and set to a very low number (6)
      * https://stackoverflow.com/a/5326159/1171533
* Reference:
    * http://nadirmuzaffar.blogspot.com/2013/03/polling-long-polling-comet-server-side.html
    * https://medium.com/javarevisited/building-scalable-facebook-like-notification-using-server-sent-event-and-redis-9d0944dee618
    * https://www.baeldung.com/spring-server-sent-events
    * https://javascript.info/server-sent-events


Web Sockets
------------
* Reference:
  * http://nadirmuzaffar.blogspot.com/2013/03/polling-long-polling-comet-server-side.html


HTTP1 vs HTTP-1.1 vs HTTP2 vs HTTP3
-----------------------------------
https://www.youtube.com/watch?v=a-sBfyiXysI

* HTTP-2: Multiple streams can be set on single TCP connection.
* Head of line blocking is an issue with HTTP -1.1 Pipelining. Also responses should be also arrive in the order they were sent.
* HTTP-2 solves the problem at application layer but the problem persists at the transport layer.
* HTTP-3: Introduced QUIC protocol where streams are first class citizens.
  * QUIC is based on UDP.
  * Each stream is independent of other streams.
  * It is used by 25% of websites.
* HTTP/2 is used around 60% of web.
* HTTP/1 is used around 25% of web.
* Google and Cloudfare have already adopted to HTTP/3 and many are in the process of adoption.

Difference between Long-Polling, Websockets, Server-Sent Events (SSE) and Comet
-------------------------------------------------------------------------------
* https://stackoverflow.com/a/12855533
* https://stackoverflow.com/a/67758338
* * My Understanding of HTTP Polling, Long Polling, HTTP Streaming and WebSockets
    https://stackoverflow.com/questions/12555043/my-understanding-of-http-polling-long-polling-http-streaming-and-websockets

References:
-------------
* Does CometD (Comet with Bayeux Protocol) use HTTP streaming or HTTP long polling?
  * https://stackoverflow.com/questions/31332016/does-cometd-comet-with-bayeux-protocol-use-http-streaming-or-http-long-polling

* What is Cometd ? Why it is used and how to work on that
  * https://stackoverflow.com/questions/27317774/what-is-cometd-why-it-is-used-and-how-to-work-on-that

* WebSockets vs. Server-Sent events/EventSource
  * https://stackoverflow.com/questions/5195452/websockets-vs-server-sent-events-eventsource

* Server client communication: Long Polling, Comet, & Server-sent Events (SSE)
  * https://stackoverflow.com/questions/10800282/server-client-communication-long-polling-comet-server-sent-events-sse


HTTP Pipelining
----------------
https://www-archive.mozilla.org/projects/netlib/http/pipelining-faq



Stream multiplexing [https://cloud.google.com/blog/products/identity-security/how-it-works-the-novel-http2-rapid-reset-ddos-attack]
-------------------

HTTP/2 uses "streams", bidirectional abstractions used to transmit various messages, or "frames", between the endpoints.
“Stream multiplexing” is the core HTTP/2 feature which allows higher utilization of each TCP connection.
Streams are multiplexed in a way that can be tracked by both sides of the connection while only using one Layer 4 connection.
Stream multiplexing enables clients to have multiple in-flight requests without managing multiple individual connections.
One of the main constraints when mounting a Layer 7 DoS attack is the number of concurrent transport connections.
Each connection carries a cost, including operating system memory for socket records and buffers, CPU time for the TLS handshake,
as well as each connection needing a unique four-tuple, the IP address and port pair for each side of the connection, constraining the number of concurrent
connections between two IP addresses.

In HTTP/1.1, each request is processed serially. The server will read a request, process it, write a response, and only then read and process the next request.
In practice, this means that the rate of requests that can be sent over a single connection is one request per round trip,
where a round trip includes the network latency, proxy processing time and backend request processing time. While HTTP/1.1 pipelining is available in some clients
 and servers to increase a connection's throughput, it is not prevalent amongst legitimate clients.

With HTTP/2, the client can open multiple concurrent streams on a single TCP connection, each stream corresponding to one HTTP request.
The maximum number of concurrent open streams is, in theory, controllable by the server, but in practice clients may open 100 streams per request and the servers
process these requests in parallel. It’s important to note that server limits can not be unilaterally adjusted.
For example, the client can open 100 streams and send a request on each of them in a single round trip; the proxy will read and process each stream serially,
but the requests to the backend servers can again be parallelized. The client can then open new streams as it receives responses to the previous ones.
This gives an effective throughput for a single connection of 100 requests per round trip, with similar round trip timing constants to HTTP/1.1 requests.
This will typically lead to almost 100 times higher utilization of each connection.


X-Forwarded-For
----------------
The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer.
 Because load balancers intercept traffic between clients and servers, your server access logs only contain the IP address of the load balancer.
 To see the IP address of the client, use the routing.http.xff_header_processing.mode attribute. This attribute enables you to modify, preserve, or remove
 the X-Forwarded-For header in the HTTP request before the Application Load Balancer sends the request to the target. The possible values for this attribute
 are append, preserve, and remove. The default value for this attribute is append.


Reference: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/x-forwarded-headers.html

X-Forwarded-Proto
------------------
The X-Forwarded-Proto request header helps you identify the protocol (HTTP or HTTPS) that a client used to connect to your load balancer.
 Your server access logs contain only the protocol used between the server and the load balancer;
 they contain no information about the protocol used between the client and the load balancer.
 To determine the protocol used between the client and the load balancer, use the X-Forwarded-Proto request header.
 Elastic Load Balancing stores the protocol used between the client and the load balancer in the X-Forwarded-Proto request header and passes the header
 along to your server.

Your application or website can use the protocol stored in the X-Forwarded-Proto request header to render a response that redirects to the appropriate URL

X-Forwarded-Port
------------------
The X-Forwarded-Port request header helps you identify the destination port that the client used to connect to the load balancer.