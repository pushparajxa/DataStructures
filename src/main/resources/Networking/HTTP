HTTP is a single request and response protocol.

Main Reference: http://nadirmuzaffar.blogspot.com/2013/03/polling-long-polling-comet-server-side.html

HTTP Persistence Connection
----------------------------
* It is also called as HTTP Keep-alive. Using this a single TCP connection is used to send multiple
http requests. Without this for each request from client to server, a connection is opened and is
closed after server sends the response. Persistence connection enables pipelining of requests
(multiple requests over single connection)

* Refer https://en.wikipedia.org/wiki/HTTP_persistent_connection

HTTP 1.0
---------
* Connections are not persistent. However un-officially it is supported by the used of header

  Connection: keep-alive

Client should send request with ahove header and if sever supports it then it will send response
with the same header. After this, connection is not dropped until client sends a request with the
header


  Connection:

  Connection: close


HTTP 1.1
--------
* All connections are persistent by-default (with timeout).

* HTTP/1.1 Spec limit number of keep-alive connections to only 2 per server by the client.

* Timeout: connection timeout of Apache httpd 1.3 and 2.0 is as little as 15 seconds and just 5
  seconds for Apache httpd 2.2 and above.
     * Refer: https://en.wikipedia.org/wiki/HTTP_persistent_connection

HTTP Pipelining
---------------
* It is available with HTTP-1.1

* It allows more than one requests to be sent over single TCP connection without waiting for their
responses. Server should send responses non-pipelined, i.e., serially in the order it has received
the requests. This may result in HOL (head of line blocking) as the initial request received by the
server may be take lot of time than others. For example, if the data is present in the server
cache then it can send immediately, otherwise, it has to query database and send the result while
other requests's data is available in the cache and can be sent quickly.


* Client sends multiple requests over the persistent connection and the server should return the
responses in the correct order the requests are received. Server need not to handle requests
concurrently.

* Idempotent requests shall only be pipelined.

* Non-Idempotent requests like POST shall not be pipelined. As per standards specification.
  - Section 8.1.1.2 Pipelining
    # https://www.w3.org/Protocols/rfc2616/rfc2616-sec8.html

* Head of line blocking (HOL)

  - One form of HOL blocking in HTTP/1.1 is when the number of allowed parallel requests in the browser is used up,
    and subsequent requests need to wait for the former ones to complete. HTTP/2 addresses this issue through request multiplexing,
    which eliminates HOL blocking at the application layer, but HOL still exists at the transport (TCP) layer.
    HTTP/3 uses QUIC instead of TCP which removes HOL blocking in the transport layer.

   - Reference:
    # https://en.wikipedia.org/wiki/Head-of-line_blocking


* Reference
  - https://en.wikipedia.org/wiki/HTTP_pipelining



HTTP/2 Streams
-------------------
The HTTP/2 specification defines a stream as “an independent, bidirectional sequence of frames exchanged between the client and server within an HTTP/2 connection.”
You can think of a stream as a series of frames making up an individual HTTP request/response pair on a connection.
When a client wants to make a request it initiates a new stream. The server will then reply on that same stream.
This is simi‐ lar to the request/response flow of h1 with the important difference that because of
the framing, multiple requests and responses can interleave together without one blocking another.
The Stream Identifier (bytes 6–9 of the frame header) is what indi‐ cates which stream a frame belongs to.


Ref: Page 47, Book: Learning HTTP/2, A Practical guide for begineers. Tech/Networking folder.


HTTP Long Polling:
-------------------
Client makes a request. After server sends a response the transaction is complete but the
underlying (TCP) (will stay unless closed by server or client) will stay open. Then client can make
another request. Server can take any amount of time (less than or equal to timeout) to send a
response.

Refer: https://stackoverflow.com/a/31335574/1171533
Timeout: connection timeout of Apache httpd 1.3 and 2.0 is as little as 15 seconds and just 5
seconds for Apache httpd 2.2 and above.
   Refer: https://en.wikipedia.org/wiki/HTTP_persistent_connection

* Long polling code example is in the package com.html.http.longpolling


HTTP Streaming (Using HTTP 1.1)
---------------
* HTTP streaming is only involves the HTTP protocol and not websockets. Streaming is also the basis for HTML5 server sent events.

COMET
------
Client makes a request. Server never ends the response. Will keep on sending messages. End of
message is identified with an identifier.

* Refer:
  * https://en.wikipedia.org/wiki/Comet_%28programming%29
  * https://infrequently.org/wp-content/LowLatencyData.pdf
  * Comet: Low Latency Data for the Browser (Alex Russel)
    - https://infrequently.org/2006/03/comet-low-latency-data-for-the-browser/
  * http://nadirmuzaffar.blogspot.com/2013/03/polling-long-polling-comet-server-side.html


      Difference between Long polling with HTTP 1.1 vs COMET
      ------------------------------------------------------
      * In comet there is single request from the client and server never ends the response. In long
        polling client will be making requests after receiving updates from it.

      * Refer http://nadirmuzaffar.blogspot.com/2013/03/polling-long-polling-comet-server-side.html

      * https://stackoverflow.com/questions/892287/advantage-of-comet-over-long-request-polling?rq=1
            [Advantage of COMET over long request polling?]


      Buffering problem with COMET and Long polling and HTTP Streaming
      ----------------------------------------------------------------
      * https://gist.github.com/CMCDragonkai/6bfade6431e9ffb7fe88

      Difference between HTTP Streaming and COMET
      -------------------------------------------
      * If you don't know the amount of data that you are going to send, i.e., live stream of a
        match, you send the data in packets/chunks, which what HTTP Streamig about. In COMET you are
        sending updates to the client from the server through single open connection.



Server Sent Events
------------------
* COMET and HTTP Streams suffer the problem of buffering. SSE will eliminate that by sending HTTP ACKs.
* Content type shall be set as below
    * Content-type: text/event-stream
* Limitations of SSE
  * Maximum open connections, which can be specially painful when opening various tabs as the limit is per browser and set to a very low number (6)
      * https://stackoverflow.com/a/5326159/1171533
* Reference:
    * http://nadirmuzaffar.blogspot.com/2013/03/polling-long-polling-comet-server-side.html
    * https://medium.com/javarevisited/building-scalable-facebook-like-notification-using-server-sent-event-and-redis-9d0944dee618
    * https://www.baeldung.com/spring-server-sent-events
    * https://javascript.info/server-sent-events


Web Sockets
------------
* Reference:
  * http://nadirmuzaffar.blogspot.com/2013/03/polling-long-polling-comet-server-side.html


HTTP1 vs HTTP-1.1 vs HTTP2 vs HTTP3
-----------------------------------
https://www.youtube.com/watch?v=a-sBfyiXysI

* HTTP-2: Multiple streams can be set on single TCP connection.
* Head of line blocking is an issue with HTTP -1.1 Pipelining. Also responses should be also arrive in the order they were sent.
* HTTP-2 solves the problem at application layer but the problem persists at the transport layer.
* HTTP-3: Introduced QUIC protocol where streams are first class citizens.
  * QUIC is based on UDP.
  * Each stream is independent of other streams.
  * It is used by 25% of websites.

Difference between Long-Polling, Websockets, Server-Sent Events (SSE) and Comet
-------------------------------------------------------------------------------
* https://stackoverflow.com/a/12855533
* https://stackoverflow.com/a/67758338
* * My Understanding of HTTP Polling, Long Polling, HTTP Streaming and WebSockets
    https://stackoverflow.com/questions/12555043/my-understanding-of-http-polling-long-polling-http-streaming-and-websockets

References:
-------------
* Does CometD (Comet with Bayeux Protocol) use HTTP streaming or HTTP long polling?
  * https://stackoverflow.com/questions/31332016/does-cometd-comet-with-bayeux-protocol-use-http-streaming-or-http-long-polling

* What is Cometd ? Why it is used and how to work on that
  * https://stackoverflow.com/questions/27317774/what-is-cometd-why-it-is-used-and-how-to-work-on-that

* WebSockets vs. Server-Sent events/EventSource
  * https://stackoverflow.com/questions/5195452/websockets-vs-server-sent-events-eventsource

* Server client communication: Long Polling, Comet, & Server-sent Events (SSE)
  * https://stackoverflow.com/questions/10800282/server-client-communication-long-polling-comet-server-sent-events-sse


HTTP Pipelining
----------------
https://www-archive.mozilla.org/projects/netlib/http/pipelining-faq



Stream multiplexing [https://cloud.google.com/blog/products/identity-security/how-it-works-the-novel-http2-rapid-reset-ddos-attack]
-------------------

HTTP/2 uses "streams", bidirectional abstractions used to transmit various messages, or "frames", between the endpoints.
“Stream multiplexing” is the core HTTP/2 feature which allows higher utilization of each TCP connection.
Streams are multiplexed in a way that can be tracked by both sides of the connection while only using one Layer 4 connection.
Stream multiplexing enables clients to have multiple in-flight requests without managing multiple individual connections.
One of the main constraints when mounting a Layer 7 DoS attack is the number of concurrent transport connections.
Each connection carries a cost, including operating system memory for socket records and buffers, CPU time for the TLS handshake,
as well as each connection needing a unique four-tuple, the IP address and port pair for each side of the connection, constraining the number of concurrent
connections between two IP addresses.

In HTTP/1.1, each request is processed serially. The server will read a request, process it, write a response, and only then read and process the next request.
In practice, this means that the rate of requests that can be sent over a single connection is one request per round trip,
where a round trip includes the network latency, proxy processing time and backend request processing time. While HTTP/1.1 pipelining is available in some clients
 and servers to increase a connection's throughput, it is not prevalent amongst legitimate clients.

With HTTP/2, the client can open multiple concurrent streams on a single TCP connection, each stream corresponding to one HTTP request.
The maximum number of concurrent open streams is, in theory, controllable by the server, but in practice clients may open 100 streams per request and the servers
process these requests in parallel. It’s important to note that server limits can not be unilaterally adjusted.
For example, the client can open 100 streams and send a request on each of them in a single round trip; the proxy will read and process each stream serially,
but the requests to the backend servers can again be parallelized. The client can then open new streams as it receives responses to the previous ones.
This gives an effective throughput for a single connection of 100 requests per round trip, with similar round trip timing constants to HTTP/1.1 requests.
This will typically lead to almost 100 times higher utilization of each connection.


X-Forwarded For
----------------
The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer.
 Because load balancers intercept traffic between clients and servers, your server access logs only contain the IP address of the load balancer.
 To see the IP address of the client, use the routing.http.xff_header_processing.mode attribute. This attribute enables you to modify, preserve, or remove
 the X-Forwarded-For header in the HTTP request before the Application Load Balancer sends the request to the target. The possible values for this attribute
 are append, preserve, and remove. The default value for this attribute is append.


Reference: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/x-forwarded-headers.html

X-Forwarded-Proto
------------------
The X-Forwarded-Proto request header helps you identify the protocol (HTTP or HTTPS) that a client used to connect to your load balancer.
 Your server access logs contain only the protocol used between the server and the load balancer;
 they contain no information about the protocol used between the client and the load balancer.
 To determine the protocol used between the client and the load balancer, use the X-Forwarded-Proto request header.
 Elastic Load Balancing stores the protocol used between the client and the load balancer in the X-Forwarded-Proto request header and passes the header
 along to your server.

Your application or website can use the protocol stored in the X-Forwarded-Proto request header to render a response that redirects to the appropriate URL

X-Forwarded-Port
------------------
The X-Forwarded-Port request header helps you identify the destination port that the client used to connect to the load balancer.